{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create anaconda environnement\n",
    "<br>\n",
    "```bash\n",
    "conda create -n ml python=3.7.4 jupyter\n",
    "```\n",
    "Install fastai library\n",
    "<br>\n",
    "```bash\n",
    "conda install -c pytorch -c fastai fastai\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Special types of matrices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identity matrix $I_{n} \\in \\mathbb{R}^{n \\times n}$ is a matrix which does not changes other matric after multiplication. This kind of matrices contain ones on main diagonal and zero everywhere else \n",
    "$$\\begin{align} I_{n} &= \\begin{pmatrix}\n",
    "           1, 0, \\dots, 0 \\\\\n",
    "           0, 1, \\dots, 0 \\\\\n",
    "           \\vdots \\\\\n",
    "           0, 0, \\dots, 1 \\\\\n",
    "         \\end{pmatrix}\n",
    "  \\end{align}$$\n",
    "<br>\n",
    "or we can define it with property $\\forall a \\in \\mathbb{R}^{1 \\times n}$ holds $aI_{n} = a$ or $\\forall a \\in \\mathbb{R}^{n \\times 1}$ holds $I_{n}a =a$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I = np.identity(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random(size=(4, 5))\n",
    "B = np.random.random(size=(6, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "I @ A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "B @ I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inverse matrix of $A \\in \\mathbb{R}^{n \\times n}$, is the matrix $A^{-1} \\in \\mathbb{R}^{n \\times n}$ for which $A^{-1}A = I$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.random.random(size=(8, 8))\n",
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "invA = np.linalg.inv(A)\n",
    "invA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ia = invA @ A\n",
    "print(Ia)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.round(Ia)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For eny $A \\in \\mathbb{R}^{n \\times m} (\\mathbb{C}^{n \\times m})$ there exists decomposition:\n",
    "$$A = U \\Sigma V^{T}$$ \n",
    "where $U \\in \\mathbb{R}^{n \\times n}(\\mathbb{C}^{n \\times n})$ is a square matrix, $\\Sigma \\in \\mathbb{R}^{n \\times m} (\\mathbb{C}^{n \\times m})$ ia a diagonal matrix and  $V \\in \\mathbb{R}^{n \\times n} (\\mathbb{C}^{n \\times n})$ is also a square matrix\n",
    "#### Note: We only discuss real valued vectors, matrices and tensors in this course"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.array([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "A, A.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "U, S, V_T = np.linalg.svd(A)\n",
    "U, S, V_T, U.shape, S.shape, V_T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Sg = np.diag(S)\n",
    "Sg, Sg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.linalg.norm(A), np.round(U[:, 0] @ U[:, 1].T), np.linalg.norm(U[2, :]), np.linalg.norm(V_T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THINGS TO ADD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Special types of matrices (I, Diagonal, Inverse)\n",
    "- Norm, Determinant\n",
    "- Eigenvalues and Eigenvectors\n",
    "- SVD\n",
    "- SVD Calculation\n",
    "- Topic modeling ?? (აქ LDA არის და არა PCA)\n",
    "- PCA\n",
    "- Faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determinant as a scaling factor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a link to the [video](https://www.youtube.com/watch?v=Ip3X9LOh2dk) about determinants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given two vectors x,y in space and some transformation matrix A. \n",
    "If we multiply these vectors by given transformation matrix, we will get transformed vectors. \n",
    "Area value before and after transformation will be changed with exactly the value of determinant of A."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](images/la2/determinant_as_scaling_factor.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eigenvalues and Eigenvectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider transformation matrix A again, and we take bunch of vectors (as much as possible) from given vector \n",
    "space where that matrix is doing transformations, we can find some vectors that never change their orientation but maybe they are scaled with some factor. \n",
    "\n",
    "Each such vector is called Eigenvector of matrix A which direction isn't affected by transformation, but scale is affected. Scale of that vector will be eigenvalue of that vector.\n",
    "\n",
    "Each eigenvector has it's eigenvalue and these can be multiple because of several axis of transformation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Eigenvalues and Eigenvectors example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](images/la2/eigenvalues_and_vectors.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Non-Eigenvalues and Eigenvectors example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](images/la2/non_eigenvalues_and_vectors.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Matrix Decompositions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[fastai LA](https://nbviewer.jupyter.org/github/fastai/numerical-linear-algebra/blob/master/nbs/1.%20Why%20are%20we%20here.ipynb#Matrix-Decompositions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[advanced matrix decompositions](https://sites.google.com/site/igorcarron2/matrixfactorizations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[nfm tutorial](https://perso.telecom-paristech.fr/essid/teach/NMF_tutorial_ICME-2014.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[topic modeling](https://medium.com/@nixalo/comp-linalg-l2-topic-modeling-with-nmf-svd-78c94330d45f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[background removal using svd](https://medium.com/@siavashmortezavi/fast-randomized-svd-singular-value-decomposition-using-pytorch-and-gpus-46b627511a6d)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "short [tutorial](http://web.mit.edu/be.400/www/SVD/Singular_Value_Decomposition.htm) on SVD from MIT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "very interesting medium [blog](https://medium.com/@jonathan_hui/machine-learning-singular-value-decomposition-svd-principal-component-analysis-pca-1d45e885e491) on SVD & PCA "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![title](images/la2/svd_steps.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Reduction\n",
    "- PCA is most commonly used to condense the information contained in a large number of original variables into a smaller set of new composite dimensions, with a minimum loss of information."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Example](https://www.projectrhea.org/rhea/index.php/PCA_Theory_Examples) of using PCA on image compression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Mapping of 2D points into 1D. \n",
    "PCA Takes the most optimal 1d axis to save data information better, reducing memory by factor of 2. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![SegmentLocal](images/la2/pca_1d.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpretation\n",
    "- PCA can be used to discover important features of a large data set. It often reveals relationships that were previously unsuspected, thereby allowing interpretations that would not ordinarily result.\n",
    "PCA is typically used as an intermediate step in data analysis when the number of input variables is otherwise too large for useful analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[example](https://towardsdatascience.com/visualising-high-dimensional-datasets-using-pca-and-t-sne-in-python-8ef87e7915b) usage of pca (and t-SNE) for data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[example](https://github.com/aviolante/sas-python-work/blob/master/tSneExampleBlogPost.ipynb) notebook for comparing PCA and t-SNE for visualizing MNIST data "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Additional Materials"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[jupyter-notebook tips&tricks&shortcuts](https://www.dataquest.io/blog/jupyter-notebook-tips-tricks-shortcuts/)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
