{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965aba33-a7f7-4c4f-ad70-110e218e3922",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff79e71-b83d-405f-8e7e-5d8eec7a00c8",
   "metadata": {},
   "source": [
    "## Organize Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d52e87f4-e8a6-4f00-99c8-e25c00c5bf1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136e7146-d005-4eda-80be-9e757ddcf86c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, random_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dff2759-e577-4017-9450-e837e0ab9d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.datasets import OxfordIIITPet\n",
    "from torchvision.transforms import Compose, Resize, ToTensor, Normalize\n",
    "import torchvision.transforms.functional as TF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd733e83-e4f1-4f32-a1b6-7f28bdb6885a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightning as pl"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "caf0802e",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section A: UNet Segmentation Training with Pretrained ResNet34 Backbone\n",
    "\n",
    "In this section we will train a UNet model for semantic segmentation. The encoder uses a pretrained ResNet34 backbone. We will use the Oxford-IIIT Pet dataset (downloaded via TorchVision) and train our UNet model using PyTorch Lightning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f2426a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformation pipeline for the dataset\n",
    "transform = Compose([\n",
    "    Resize((128, 128)),            # Resize images to 128x128\n",
    "    ToTensor(),                    # Convert PIL image to tensor\n",
    "    Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "target_trf = Compose([\n",
    "    Resize((128, 128)),            # Resize images to 128x128\n",
    "    ToTensor(),                    # Convert PIL image to tensor\n",
    "])\n",
    "\n",
    "# Download and load the Oxford-IIIT Pet dataset (for segmentation)\n",
    "pet_dataset = OxfordIIITPet(\n",
    "    root=\"oxford-iiit-pet\", \n",
    "    download=True, \n",
    "    target_types=\"segmentation\", \n",
    "    transform=transform,\n",
    "    target_transform=target_trf\n",
    ")\n",
    "print(\"Oxford-IIIT Pet dataset loaded:\", len(pet_dataset), \"samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1af26d4-d443-4fe5-bed7-9b10114a2800",
   "metadata": {},
   "outputs": [],
   "source": [
    "img, trg = pet_dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cbe9ee4-f912-453f-9309-3168aba389e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "img.shape, trg.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57aedcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PretrainedUNet(nn.Module):\n",
    "    def __init__(self, n_class=1):\n",
    "        super().__init__()\n",
    "        # Load a pretrained ResNet34 model\n",
    "        resnet = models.resnet34(models.resnet.ResNet34_Weights.DEFAULT)\n",
    "        \n",
    "        # comment: Use early layers as the encoder\n",
    "        self.encoder0 = nn.Sequential(resnet.conv1, resnet.bn1, resnet.relu)  # [B, 64, H/2, W/2]\n",
    "        self.pool0 = resnet.maxpool  # Reduces spatial size by factor of 2\n",
    "        self.encoder1 = resnet.layer1  # [B, 64, H/4, W/4]\n",
    "        self.encoder2 = resnet.layer2  # [B, 128, H/8, W/8]\n",
    "        self.encoder3 = resnet.layer3  # [B, 256, H/16, W/16]\n",
    "        self.encoder4 = resnet.layer4  # [B, 512, H/32, W/32]\n",
    "        \n",
    "        # comment: Decoder layers with upsampling and skip connections\n",
    "        self.upconv4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n",
    "        self.decoder4 = self.double_conv(512, 256)  # Concatenate with encoder3\n",
    "        \n",
    "        self.upconv3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n",
    "        self.decoder3 = self.double_conv(256, 128)  # Concatenate with encoder2\n",
    "        \n",
    "        self.upconv2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n",
    "        self.decoder2 = self.double_conv(128, 64)   # Concatenate with encoder1\n",
    "        \n",
    "        self.upconv1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n",
    "        self.decoder1 = self.double_conv(128, 64)     # Concatenate with encoder0\n",
    "        \n",
    "        self.out_conv = nn.Conv2d(64, n_class, kernel_size=1)  # Output layer\n",
    "        \n",
    "    def double_conv(self, in_channels, out_channels):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        x0 = self.encoder0(x)        \n",
    "        x1 = self.pool0(x0)          \n",
    "        x1 = self.encoder1(x1)       \n",
    "        x2 = self.encoder2(x1)       \n",
    "        x3 = self.encoder3(x2)       \n",
    "        x4 = self.encoder4(x3)       \n",
    "        \n",
    "        # comment: Bottleneck is implicit in x4\n",
    "        \n",
    "        # comment: Decoder\n",
    "        d4 = self.upconv4(x4)        \n",
    "        d4 = torch.cat([d4, x3], dim=1)  \n",
    "        d4 = self.decoder4(d4)       \n",
    "        \n",
    "        d3 = self.upconv3(d4)        \n",
    "        d3 = torch.cat([d3, x2], dim=1)  \n",
    "        d3 = self.decoder3(d3)       \n",
    "        \n",
    "        d2 = self.upconv2(d3)        \n",
    "        d2 = torch.cat([d2, x1], dim=1)  \n",
    "        d2 = self.decoder2(d2)       \n",
    "        \n",
    "        d1 = self.upconv1(d2)        \n",
    "        d1 = torch.cat([d1, x0], dim=1)  \n",
    "        d1 = self.decoder1(d1)       \n",
    "        \n",
    "        out = self.out_conv(d1)\n",
    "        out = F.interpolate(out, size=(x.shape[2], x.shape[3]), mode='bilinear', align_corners=False)\n",
    "        \n",
    "        return out\n",
    "\n",
    "# Instantiate UNet with 1 output channel (binary segmentation)\n",
    "unet_model = PretrainedUNet(n_class=1)\n",
    "print(unet_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b43ef81",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UNetLightning(pl.LightningModule):\n",
    "    def __init__(self, lr=1e-3):\n",
    "        super(UNetLightning, self).__init__()\n",
    "        self.model = unet_model  # comment: Using the UNet defined above\n",
    "        self.lr = lr\n",
    "        self.criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model(x)\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        imgs, masks = batch\n",
    "        imgs = imgs.to(self.device)\n",
    "        # Assuming masks are already transformed to tensors\n",
    "        masks = masks.to(self.device).float()\n",
    "        outputs = self.model(imgs)\n",
    "        loss = self.criterion(outputs, masks)\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        imgs, masks = batch\n",
    "        imgs = imgs.to(self.device)\n",
    "        masks = masks.to(self.device).float()\n",
    "        outputs = self.model(imgs)\n",
    "        loss = self.criterion(outputs, masks)\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = optim.Adam(self.model.parameters(), lr=self.lr)\n",
    "        return optimizer\n",
    "\n",
    "# Instantiate the Lightning module for UNet training\n",
    "unet_lightning = UNetLightning(lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c56b6fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the Oxford-IIIT Pet dataset into train and validation subsets\n",
    "train_size = int(0.8 * len(pet_dataset))\n",
    "val_size = len(pet_dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(pet_dataset, [train_size, val_size])\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True, num_workers=0)\n",
    "val_loader = DataLoader(val_dataset, batch_size=8, shuffle=False, num_workers=0)\n",
    "\n",
    "print(f\"Training samples: {len(train_dataset)}, Validation samples: {len(val_dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0cbbcb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a PyTorch Lightning Trainer for UNet\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5, \n",
    "    accelerator='auto', \n",
    "    devices=1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e35e3f5c-8963-48d4-8c9c-e224920c302a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader.dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507eed10-a190-4688-98cb-23023ad41b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the UNet model\n",
    "trainer.fit(unet_lightning, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1ad92ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "unet_lightning.model.eval()\n",
    "# Get one batch from the validation loader\n",
    "batch = next(iter(val_loader))\n",
    "imgs, true_masks = batch\n",
    "imgs = imgs.to(unet_lightning.device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    preds = unet_lightning.model(imgs)\n",
    "\n",
    "# Visualize the first image's prediction\n",
    "img_tensor = imgs[0].cpu()\n",
    "true_mask = true_masks[0].cpu().squeeze()  # comment: Assuming single channel mask\n",
    "pred_mask = torch.sigmoid(preds[0]).cpu().squeeze()\n",
    "pred_mask_bin = (pred_mask > 0.5).float()\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,3,1)\n",
    "plt.imshow(TF.to_pil_image(img_tensor))\n",
    "plt.title(\"Input Image\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,2)\n",
    "plt.imshow(true_mask, cmap='gray')\n",
    "plt.title(\"True Mask\")\n",
    "plt.axis(\"off\")\n",
    "\n",
    "plt.subplot(1,3,3)\n",
    "plt.imshow(pred_mask_bin, cmap='gray')\n",
    "plt.title(\"Predicted Mask\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d28de5",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section B: Mask-RCNN Instance Segmentation Training with PyTorch Lightning\n",
    "\n",
    "In this section we will train a Mask-RCNN model on the PennFudanPed dataset. We download and extract the dataset, define a custom Dataset class and DataLoaders, then wrap TorchVision’s Mask-RCNN in a PyTorch Lightning module for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19bac82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, zipfile, io\n",
    "\n",
    "url = \"https://www.cis.upenn.edu/~jshi/ped_html/PennFudanPed.zip\"\n",
    "response = requests.get(url)\n",
    "if response.status_code == 200:\n",
    "    with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n",
    "        z.extractall(\"PennFudanPed\")\n",
    "    print(\"PennFudanPed dataset downloaded and extracted to './PennFudanPed'\")\n",
    "else:\n",
    "    print(\"Download failed with status code:\", response.status_code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f4e4c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "class PennFudanDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, root, transforms=None):\n",
    "        self.root = root\n",
    "        self.transforms = transforms\n",
    "        self.imgs = list(sorted(os.listdir(os.path.join(root, \"PNGImages\"))))\n",
    "        self.masks = list(sorted(os.listdir(os.path.join(root, \"PedMasks\"))))\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.root, \"PNGImages\", self.imgs[idx])\n",
    "        mask_path = os.path.join(self.root, \"PedMasks\", self.masks[idx])\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        mask = Image.open(mask_path)\n",
    "        mask = np.array(mask)\n",
    "        obj_ids = np.unique(mask)[1:]  # comment: Remove background\n",
    "        masks = mask == obj_ids[:, None, None]\n",
    "        boxes = []\n",
    "        for i in range(len(obj_ids)):\n",
    "            pos = np.where(masks[i])\n",
    "            xmin = np.min(pos[1])\n",
    "            xmax = np.max(pos[1])\n",
    "            ymin = np.min(pos[0])\n",
    "            ymax = np.max(pos[0])\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        num_objs = len(obj_ids)\n",
    "        labels = torch.ones((num_objs,), dtype=torch.int64)  # comment: one class (person)\n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = torch.tensor([idx])\n",
    "        if self.transforms is not None:\n",
    "            img = self.transforms(img)\n",
    "        return img, target\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "\n",
    "dataset_full = PennFudanDataset(\"PennFudanPed\", transforms=None)\n",
    "n = len(dataset_full)\n",
    "n_train = int(0.8 * n)\n",
    "n_val = n - n_train\n",
    "dataset_train, dataset_val = random_split(dataset_full, [n_train, n_val])\n",
    "\n",
    "train_loader = DataLoader(dataset_train, batch_size=2, shuffle=True, num_workers=0, collate_fn=collate_fn)\n",
    "val_loader = DataLoader(dataset_val, batch_size=1, shuffle=False, num_workers=0, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff23c295",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "import torchvision.models.detection as detection\n",
    "\n",
    "class MaskRCNNLightning(pl.LightningModule):\n",
    "    def __init__(self, num_classes=2, lr=0.005):\n",
    "        super().__init__()\n",
    "        # Load pre-trained Mask-RCNN\n",
    "        self.model = detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
    "        in_features = self.model.roi_heads.box_predictor.cls_score.in_features\n",
    "        self.model.roi_heads.box_predictor = detection.faster_rcnn.FastRCNNPredictor(in_features, num_classes)\n",
    "        self.lr = lr\n",
    "\n",
    "    def forward(self, images, targets=None):\n",
    "        return self.model(images, targets)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        images = [img.to(self.device) for img in images]\n",
    "        targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = self.model(images, targets)\n",
    "        loss = sum(loss.mean() for loss in loss_dict.values())\n",
    "        self.log(\"train_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        images, targets = batch\n",
    "        images = [img.to(self.device) for img in images]\n",
    "        targets = [{k: v.to(self.device) for k, v in t.items()} for t in targets]\n",
    "        loss_dict = self.model(images, targets)\n",
    "        loss = sum(loss.mean() for loss in loss_dict.values())\n",
    "        self.log(\"val_loss\", loss, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.SGD(self.model.parameters(), lr=self.lr, momentum=0.9, weight_decay=0.0005)\n",
    "        scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)\n",
    "        return [optimizer], [scheduler]\n",
    "\n",
    "maskrcnn_model = MaskRCNNLightning(num_classes=2, lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c422d936",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl\n",
    "\n",
    "trainer = pl.Trainer(max_epochs=3, accelerator=\"auto\", devices=1)\n",
    "trainer.fit(maskrcnn_model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4294770",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torchvision.transforms.functional as TF\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "\n",
    "maskrcnn_model.model.eval()\n",
    "batch = next(iter(val_loader))\n",
    "images, targets = batch\n",
    "images = [img.to(maskrcnn_model.device) for img in images]\n",
    "\n",
    "with torch.no_grad():\n",
    "    outputs = maskrcnn_model.model(images)\n",
    "\n",
    "# Visualize predictions for the first image in the batch\n",
    "img = images[0].cpu()\n",
    "img_vis = (img * 255).type(torch.uint8)\n",
    "scores = outputs[0][\"scores\"].detach().cpu()\n",
    "keep = scores >= 0.5\n",
    "boxes = outputs[0][\"boxes\"][keep].detach().cpu()\n",
    "\n",
    "drawn_img = draw_bounding_boxes(img_vis, boxes, colors=\"red\", width=2)\n",
    "\n",
    "plt.figure(figsize=(8,8))\n",
    "plt.imshow(TF.to_pil_image(drawn_img))\n",
    "plt.axis(\"off\")\n",
    "plt.title(\"Mask-RCNN Validation Predictions\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
