{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:23.993097Z",
     "start_time": "2018-08-25T03:56:23.138209Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.006246Z",
     "start_time": "2018-08-25T03:56:23.998204Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.style.use(['ggplot'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5> Generate some data with:\n",
    "\\begin{equation} \\theta_0= 4 \\end{equation} \n",
    "\\begin{equation} \\theta_1= 3 \\end{equation} \n",
    "\n",
    "Add some Gaussian noise to the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have $$y=\\theta_0 + \\theta_1 *X+ random\\ noise$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.027559Z",
     "start_time": "2018-08-25T03:56:24.016052Z"
    }
   },
   "outputs": [],
   "source": [
    "X = 2 * np.random.rand(100,1)\n",
    "y = 4 +3 * X+np.random.randn(100,1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot our data to check the relation between X and Y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot generated 2d data points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.780299Z",
     "start_time": "2018-08-25T03:56:24.032204Z"
    }
   },
   "outputs": [],
   "source": [
    "plt.plot(X,y,'b.')\n",
    "plt.xlabel(\"$x$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "ax =plt.axis([0,2,0,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  Analytical way of Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ?? np.c_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.792752Z",
     "start_time": "2018-08-25T03:56:24.783380Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X_b = np.c_[np.ones((100,1)),X]; X_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normal Equation    \n",
    "\n",
    "$\\hat{\\boldsymbol\\beta} = (\\mathbf{X}^\\mathsf{T}\\mathbf{X})^{-1} \\mathbf{X}^\\mathsf{T} \\mathbf{y}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.792752Z",
     "start_time": "2018-08-25T03:56:24.783380Z"
    }
   },
   "outputs": [],
   "source": [
    "theta_best = np.linalg.inv(X_b.T.dot(X_b)).dot(X_b.T).dot(y);\n",
    "theta_best"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>This is close to our real thetas 4 and 3. It cannot be accurate due to the noise I have introduced in data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:24.805370Z",
     "start_time": "2018-08-25T03:56:24.795517Z"
    }
   },
   "outputs": [],
   "source": [
    "X_new = np.array([[0],[2]])\n",
    "X_new_b = np.c_[np.ones((2,1)),X_new]\n",
    "y_predict = X_new_b.dot(theta_best)\n",
    "y_predict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h5>Let's plot prediction line with calculated:theta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:25.168502Z",
     "start_time": "2018-08-25T03:56:24.808124Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.plot(X_new,y_predict,'r-')\n",
    "plt.plot(X,y,'b.')\n",
    "plt.xlabel(\"$x_1$\", fontsize=18)\n",
    "plt.ylabel(\"$y$\", rotation=0, fontsize=18)\n",
    "plt.axis([0,2,0,15])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Function & Gradients\n",
    "\n",
    "<h4> The equation for calculating cost function and gradients are as shown below. Please note the cost function is for Linear regression. For other algorithms the cost function will be different and the gradients would have  to be derived from the cost functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<b>Cost</b>\n",
    "\\begin{equation}\n",
    "J(\\theta) = 1/2m \\sum_{i=1}^{m} (h(\\theta)^{(i)} - y^{(i)})^2 \n",
    "\\end{equation}\n",
    "\n",
    "<b>Gradient</b>\n",
    "\n",
    "\\begin{equation}\n",
    "\\frac{\\partial J(\\theta)}{\\partial \\theta_j} = 1/m\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_j^{(i)}\n",
    "\\end{equation}\n",
    "\n",
    "<b>Gradients</b>\n",
    "\\begin{equation}\n",
    "\\theta_0: = \\theta_0 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_0^{(i)})\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta_1: = \\theta_1 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_1^{(i)})\n",
    "\\end{equation}\n",
    "\\begin{equation}\n",
    "\\theta_2: = \\theta_2 -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_2^{(i)})\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "\\theta_j: = \\theta_j -\\alpha . (1/m .\\sum_{i=1}^{m}(h(\\theta^{(i)} - y^{(i)}).X_j^{(i)})\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:25.180630Z",
     "start_time": "2018-08-25T03:56:25.172904Z"
    }
   },
   "outputs": [],
   "source": [
    "def  cal_cost(theta,X,y):\n",
    "    '''\n",
    "    \n",
    "    Calculates the cost for given X and Y. The following shows and example of a single dimensional X\n",
    "    theta = Vector of thetas \n",
    "    X     = Row of X's np.zeros((2,j))\n",
    "    y     = Actual y's np.zeros((2,1))\n",
    "    \n",
    "    where:\n",
    "        j is the no of features\n",
    "    '''\n",
    "    \n",
    "    m = len(y)\n",
    "    \n",
    "    predictions = X.dot(theta)\n",
    "    cost = (1/2*m) * np.sum(np.square(predictions-y))\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:25.204130Z",
     "start_time": "2018-08-25T03:56:25.188272Z"
    }
   },
   "outputs": [],
   "source": [
    "def gradient_descent(X,y,theta,learning_rate=0.01,iterations=100):\n",
    "    '''\n",
    "    X    = Matrix of X with added bias units\n",
    "    y    = Vector of Y\n",
    "    theta=Vector of thetas np.random.randn(j,1)\n",
    "    learning_rate \n",
    "    iterations = no of iterations\n",
    "    \n",
    "    Returns the final theta vector and array of cost history over no of iterations\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    theta_history = np.zeros((iterations,2))\n",
    "    for it in range(iterations):\n",
    "        \n",
    "        prediction = np.dot(X,theta)\n",
    "        \n",
    "        theta = theta -(1/m)*learning_rate*( X.T.dot((prediction - y)))\n",
    "        theta_history[it,:] =theta.T\n",
    "        cost_history[it]  = cal_cost(theta,X,y)\n",
    "        \n",
    "    return theta, cost_history, theta_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's start with 1000 iterations and a learning rate of 0.01. Start with theta from a Gaussian distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T03:56:27.126960Z",
     "start_time": "2018-08-25T03:56:27.062811Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "lr =1.1\n",
    "n_iter = 1000\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "theta,cost_history,theta_history = gradient_descent(X_b,y,theta,lr,n_iter)\n",
    "\n",
    "\n",
    "print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n",
    "print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> Let's plot the cost history over iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T04:01:31.912400Z",
     "start_time": "2018-08-25T04:01:31.604459Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(12,8))\n",
    "\n",
    "ax.set_ylabel('J(Theta)')\n",
    "ax.set_xlabel('Iterations')\n",
    "_=ax.plot(range(n_iter),cost_history,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3> After around 150 iterations the cost is flat so the remaining iterations  are not needed or will not result in any further optimization. Let us zoom in till iteration 200 and see the curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:22:30.561842Z",
     "start_time": "2018-08-19T05:22:30.371532Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "_=ax.plot(range(200),cost_history[:200],'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>It is worth while to note that the cost drops faster initially and then the gain in cost reduction is not as much"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It would be great to see the effect of different learning rates and iterations together"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let us  build a function which can show the effects together and also show how gradient decent actually is working"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T04:06:54.948007Z",
     "start_time": "2018-08-25T04:06:54.916435Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_GD(n_iter,lr,ax,ax1=None):\n",
    "     \"\"\"\n",
    "     n_iter = no of iterations\n",
    "     lr = Learning Rate\n",
    "     ax = Axis to plot the Gradient Descent\n",
    "     ax1 = Axis to plot cost_history vs Iterations plot\n",
    "\n",
    "     \"\"\"\n",
    "     _ = ax.plot(X,y,'b.')\n",
    "     theta = np.random.randn(2,1)\n",
    "\n",
    "     tr =0.1\n",
    "     cost_history = np.zeros(n_iter)\n",
    "     for i in range(n_iter):\n",
    "        pred_prev = X_b.dot(theta)\n",
    "        theta,h,_ = gradient_descent(X_b,y,theta,lr,1)\n",
    "        pred = X_b.dot(theta)\n",
    "\n",
    "        cost_history[i] = h[0]\n",
    "\n",
    "        if ((i % 25 == 0) ):\n",
    "            _ = ax.plot(X,pred,'r-',alpha=tr)\n",
    "            if tr < 0.8:\n",
    "                tr = tr+0.2\n",
    "     if not ax1== None:\n",
    "        _ = ax1.plot(range(n_iter),cost_history,'b.')  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the graphs for different iterations and learning rates combination"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-25T04:07:09.977910Z",
     "start_time": "2018-08-25T04:07:00.484794Z"
    }
   },
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(30,25),dpi=200)\n",
    "fig.subplots_adjust(hspace=0.4, wspace=0.4)\n",
    "\n",
    "it_lr =[(2000,0.001),(500,0.01),(200,0.05),(100,0.1)]\n",
    "count =0\n",
    "for n_iter, lr in it_lr:\n",
    "    count += 1\n",
    "    \n",
    "    ax = fig.add_subplot(4, 2, count)\n",
    "    count += 1\n",
    "   \n",
    "    ax1 = fig.add_subplot(4,2,count)\n",
    "    \n",
    "    ax.set_title(\"lr:{}\".format(lr))\n",
    "    ax1.set_title(\"Iterations:{}\".format(n_iter))\n",
    "    plot_GD(n_iter,lr,ax,ax1)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> See how useful it is to visualize the effect of learning rates and iterations on gradient descent. The red lines show how the gradient descent starts and then slowly gets closer to the final value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-17T09:38:15.809824Z",
     "start_time": "2018-08-17T09:38:15.807296Z"
    }
   },
   "source": [
    "## You can always plot Indiviual graphs to zoom in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:14.735491Z",
     "start_time": "2018-08-19T05:23:14.487772Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "_,ax = plt.subplots(figsize=(14,10))\n",
    "plot_GD(100,0.1,ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stochastic Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:17.900469Z",
     "start_time": "2018-08-19T05:23:17.882614Z"
    }
   },
   "outputs": [],
   "source": [
    "def stocashtic_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10):\n",
    "    '''\n",
    "    X    = Matrix of X with added bias units\n",
    "    y    = Vector of Y\n",
    "    theta=Vector of thetas np.random.randn(j,1)\n",
    "    learning_rate \n",
    "    iterations = no of iterations\n",
    "    \n",
    "    Returns the final theta vector and array of cost history over no of iterations\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    \n",
    "    \n",
    "    for it in range(iterations):\n",
    "        cost =0.0\n",
    "        for i in range(m):\n",
    "            rand_ind = np.random.randint(0,m)\n",
    "            X_i = X[rand_ind,:].reshape(1,X.shape[1])\n",
    "            y_i = y[rand_ind].reshape(1,1)\n",
    "            prediction = np.dot(X_i,theta)\n",
    "\n",
    "            theta = theta -(1/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n",
    "            cost += cal_cost(theta,X_i,y_i)\n",
    "        cost_history[it]  = cost\n",
    "        \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:20.091678Z",
     "start_time": "2018-08-19T05:23:19.910817Z"
    }
   },
   "outputs": [],
   "source": [
    "lr =0.5\n",
    "n_iter = 50\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "X_b = np.c_[np.ones((len(X),1)),X]\n",
    "theta,cost_history = stocashtic_gradient_descent(X_b,y,theta,lr,n_iter)\n",
    "\n",
    "\n",
    "print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n",
    "print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:20.736824Z",
     "start_time": "2018-08-19T05:23:20.484570Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.set_ylabel('{J(Theta)}',rotation=0)\n",
    "ax.set_xlabel('{Iterations}')\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "_=ax.plot(range(n_iter),cost_history,'b.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mini Batch Gradient Descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:25.240288Z",
     "start_time": "2018-08-19T05:23:25.217657Z"
    }
   },
   "outputs": [],
   "source": [
    "def minibatch_gradient_descent(X,y,theta,learning_rate=0.01,iterations=10,batch_size =20):\n",
    "    '''\n",
    "    X    = Matrix of X without added bias units\n",
    "    y    = Vector of Y\n",
    "    theta=Vector of thetas np.random.randn(j,1)\n",
    "    learning_rate \n",
    "    iterations = no of iterations\n",
    "    \n",
    "    Returns the final theta vector and array of cost history over no of iterations\n",
    "    '''\n",
    "    m = len(y)\n",
    "    cost_history = np.zeros(iterations)\n",
    "    n_batches = int(m/batch_size)\n",
    "    \n",
    "    for it in range(iterations):\n",
    "        cost =0.0\n",
    "        indices = np.random.permutation(m)\n",
    "        X = X[indices]\n",
    "        y = y[indices]\n",
    "        for i in range(0,m,batch_size):\n",
    "            X_i = X[i:i+batch_size]\n",
    "            y_i = y[i:i+batch_size]\n",
    "            \n",
    "            X_i = np.c_[np.ones(len(X_i)),X_i]\n",
    "           \n",
    "            prediction = np.dot(X_i,theta)\n",
    "\n",
    "            theta = theta -(1/m)*learning_rate*( X_i.T.dot((prediction - y_i)))\n",
    "            cost += cal_cost(theta,X_i,y_i)\n",
    "        cost_history[it]  = cost\n",
    "        \n",
    "    return theta, cost_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:25.835249Z",
     "start_time": "2018-08-19T05:23:25.724421Z"
    }
   },
   "outputs": [],
   "source": [
    "lr =0.1\n",
    "n_iter = 200\n",
    "\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "\n",
    "theta,cost_history = minibatch_gradient_descent(X,y,theta,lr,n_iter)\n",
    "\n",
    "\n",
    "print('Theta0:          {:0.3f},\\nTheta1:          {:0.3f}'.format(theta[0][0],theta[1][0]))\n",
    "print('Final cost/MSE:  {:0.3f}'.format(cost_history[-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-08-19T05:23:26.512988Z",
     "start_time": "2018-08-19T05:23:26.326207Z"
    }
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,8))\n",
    "\n",
    "ax.set_ylabel('{J(Theta)}',rotation=0)\n",
    "ax.set_xlabel('{Iterations}')\n",
    "theta = np.random.randn(2,1)\n",
    "\n",
    "_=ax.plot(range(n_iter),cost_history,'b.')"
   ]
  }
 ],
 "metadata": {
  "gist": {
   "data": {
    "description": "Gradient Descent-Python",
    "public": true
   },
   "id": ""
  },
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "279.233px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "461.183px",
    "left": "846.167px",
    "right": "138.333px",
    "top": "127px",
    "width": "559.667px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
